{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29cc94fa-b6f1-43a6-a124-cea138b586da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. What is Bayes' theorem?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#Bayes' theorem, named after the Reverend Thomas Bayes, is a fundamental principle in probability theory and statistics. It describes how to update the probability of an event based on new evidence or information. Bayes' theorem mathematically relates two types of probabilities: the conditional probability and the prior probability.\n",
    "\n",
    "#The theorem can be stated as follows:\n",
    "\n",
    "#P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "#Where:\n",
    "\n",
    "#P(A|B) is the posterior probability, which is the probability of event A occurring given that event B has occurred.\n",
    "#P(B|A) is the conditional probability, which is the probability of event B occurring given that event A has occurred.\n",
    "#P(A) is the prior probability, which is the probability of event A occurring before any new evidence.\n",
    "#P(B) is the probability of event B occurring before any new evidence.\n",
    "#In simple terms, Bayes' theorem allows us to update our beliefs or probabilities about an event A, given new evidence in the form of event B. It provides a framework to incorporate new information and revise our initial beliefs or estimates.\n",
    "\n",
    "#The theorem has applications in various fields, including statistics, machine learning, medical diagnosis, and spam filtering. It is especially useful when we want to make inferences based on incomplete or uncertain information, by iteratively updating our beliefs as new evidence becomes available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdaeebfa-acc2-4520-a59a-083aaff5ea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. What is the formula for Bayes' theorem?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#The formula for Bayes' theorem is as follows:\n",
    "\n",
    "#P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "#Where:\n",
    "\n",
    "#P(A|B) represents the posterior probability of event A occurring given that event B has occurred.\n",
    "#P(B|A) represents the conditional probability of event B occurring given that event A has occurred.\n",
    "#P(A) represents the prior probability of event A occurring before any new evidence.\n",
    "#P(B) represents the probability of event B occurring before any new evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfe127c6-27cf-4166-a487-b218f7dd9e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. How is Bayes' theorem used in practice?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#Bayes' theorem is widely used in various fields and applications. Here are some practical examples of how Bayes' theorem is used:\n",
    "\n",
    "#1 - Medical diagnosis: Bayes' theorem is used in medical diagnosis to update the probability of a disease or condition based on test results. For example, if a patient tests positive for a particular disease, Bayes' theorem can be applied to calculate the probability of actually having the disease, taking into account the sensitivity and specificity of the test.\n",
    "\n",
    "#2 - Spam filtering: Bayes' theorem is employed in spam filters to classify incoming emails as either spam or legitimate. The theorem helps update the probability of an email being spam or not, based on the occurrence of certain words or patterns in the email. This approach is known as \"Naive Bayes\" and is widely used in text classification tasks.\n",
    "\n",
    "#3 - Predictive modeling: Bayes' theorem is used in Bayesian statistics and Bayesian inference to update prior beliefs about a parameter or model based on observed data. It allows for the incorporation of new information into the modeling process and provides a framework for making predictions and estimating uncertainties.\n",
    "\n",
    "#4 - Risk assessment: Bayes' theorem is employed in risk assessment to update the probability of an event occurring based on new evidence or information. It helps in quantifying and evaluating risks by combining prior knowledge or beliefs with observed data or expert opinions.\n",
    "\n",
    "#5 - Machine learning: Bayes' theorem serves as a foundation for various machine learning algorithms, such as Naive Bayes classifiers and Bayesian networks. These methods use Bayes' theorem to make predictions or classify data based on prior probabilities and observed evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28c03dda-a80b-4199-bd8c-f99023ca448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. What is the relationship between Bayes' theorem and conditional probability?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#Bayes' theorem is closely related to conditional probability. In fact, Bayes' theorem can be derived from the definition of conditional probability.\n",
    "\n",
    "#Conditional probability is the probability of an event A occurring given that event B has occurred, denoted as P(A|B). It is calculated as:\n",
    "\n",
    "#P(A|B) = P(A and B) / P(B)\n",
    "\n",
    "#Bayes' theorem provides a way to invert this relationship. It allows us to calculate the probability of event B occurring given that event A has occurred, by incorporating prior probabilities. The theorem can be stated as:\n",
    "\n",
    "#P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "#Where:\n",
    "\n",
    "#P(A|B) is the posterior probability, the probability of event A occurring given that event B has occurred.\n",
    "#P(B|A) is the conditional probability, the probability of event B occurring given that event A has occurred.\n",
    "#P(A) is the prior probability, the probability of event A occurring before any new evidence.\n",
    "#P(B) is the probability of event B occurring before any new evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6e5d111-4be4-4a00-b035-ee3e03fbe580",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. How do you choose which type of Naive Bayes classifier to use for any given problem?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#When choosing the type of Naive Bayes classifier to use for a given problem, it is essential to consider the specific characteristics of the data and the assumptions made by each classifier variant. There are three common types of Naive Bayes classifiers: Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes. Here are some considerations for selecting the appropriate type:\n",
    "\n",
    "#1 - Nature of the features: The first consideration is the nature of the features in your dataset.\n",
    "\n",
    "#Gaussian Naive Bayes assumes that the features follow a Gaussian (normal) distribution. It is suitable for continuous or real-valued features.\n",
    "\n",
    "#Multinomial Naive Bayes is appropriate when dealing with discrete features, such as word counts or categorical variables, typically encountered in text classification or document analysis tasks.\n",
    "\n",
    "#Bernoulli Naive Bayes is useful when dealing with binary features, where each feature can take on a value of 0 or 1. It is often applied in problems like sentiment analysis, spam detection, or document classification with binary term presence/absence features.\n",
    "\n",
    "#2 - Independence assumption: Naive Bayes classifiers assume independence between features given the class label. While this assumption might not hold in all cases, it is often reasonable and simplifies the modeling process. However, if there are strong dependencies or interactions between features in your dataset, other classifiers might be more suitable.\n",
    "\n",
    "#3 - Dataset size and sparsity: The size and sparsity of the dataset can also influence the choice of the Naive Bayes variant.\n",
    "\n",
    "#Gaussian Naive Bayes requires estimating the mean and variance of each feature for each class. It can handle small to medium-sized datasets but may struggle with high-dimensional or sparse data due to the risk of overfitting.\n",
    "\n",
    "#Multinomial Naive Bayes is commonly used for text classification tasks with large, sparse feature spaces, such as document classification, sentiment analysis, or spam detection. It handles the presence of multiple occurrences of features.\n",
    "\n",
    "#Bernoulli Naive Bayes is appropriate for binary feature spaces and works well with sparse data. It is particularly suited for problems with binary term presence/absence features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e4493d1-afcc-48c9-bf98-9edd70f38182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Assignment:\n",
    "\n",
    "#You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of each feature value for each class:\n",
    "\n",
    "#Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
    "\n",
    "#A 3 3 4 4 3 3 3\n",
    "\n",
    "#B 2 2 1 2 2 2 3\n",
    "\n",
    "#Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance to belong to?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#To predict the class for the new instance using Naive Bayes, we need to calculate the conditional probabilities of each class given the observed feature values (X1 = 3 and X2 = 4). Here's how we can proceed:\n",
    "\n",
    "#Step 1: Calculate the prior probabilities of each class. Since the prior probabilities are assumed to be equal, P(A) = P(B) = 0.5.\n",
    "\n",
    "#Step 2: Calculate the conditional probabilities of each class given the observed feature values (X1 = 3 and X2 = 4).\n",
    "\n",
    "#For Class A:\n",
    "#P(X1 = 3 | A) = 4/13\n",
    "#P(X2 = 4 | A) = 3/13\n",
    "\n",
    "#For Class B:\n",
    "#P(X1 = 3 | B) = 1/10\n",
    "#P(X2 = 4 | B) = 3/10\n",
    "\n",
    "#Step 3: Calculate the joint probabilities of each class given the observed feature values using the Naive Bayes assumption of independence:\n",
    "\n",
    "#For Class A:\n",
    "#P(A | X1 = 3, X2 = 4) = P(X1 = 3 | A) * P(X2 = 4 | A) * P(A)\n",
    "\n",
    "#For Class B:\n",
    "#P(B | X1 = 3, X2 = 4) = P(X1 = 3 | B) * P(X2 = 4 | B) * P(B)\n",
    "\n",
    "#Step 4: Normalize the joint probabilities to obtain the posterior probabilities:\n",
    "\n",
    "#Normalize P(A | X1 = 3, X2 = 4) and P(B | X1 = 3, X2 = 4) by dividing each by their sum.\n",
    "\n",
    "#Now we can calculate the probabilities:\n",
    "\n",
    "#P(A | X1 = 3, X2 = 4) = (4/13) * (3/13) * (0.5) = 0.0468\n",
    "#P(B | X1 = 3, X2 = 4) = (1/10) * (3/10) * (0.5) = 0.015\n",
    "\n",
    "#Since P(A | X1 = 3, X2 = 4) is greater than P(B | X1 = 3, X2 = 4), the Naive Bayes classifier would predict the new instance to belong to Class A.\n",
    "\n",
    "#Therefore, Naive Bayes would predict the new instance to belong to Class A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b028c339-8a9b-4641-80a1-388b8a1f18a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
